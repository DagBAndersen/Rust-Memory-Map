


- Gennemsøgning så speeder træet op.
- 30 procent cache miss for store datamængder - 60procent for mindre datamængder.
- tree 20% hurtigere
- tabellen er fuld random lookup, så ingen pattern. Tree vil altid have the upper nodes in cache, meaning 20 increase
- 8 gb maskine caper ud ved 270mb - idle på 140mb -> 130 mb diff
 - 2 gb maskiner capper ud ved 200mb - idle på 80mb -> 120 mb diff

----------------------------------------------------------------------------------------------------
- efter kun 2 ticks er den oppe på 270 mb


dionysos til sidst i tree build before and : 25477976-25243404  254mb


- hvis jeg låser nodesne til redblack tree'et så dør den efter 10_000 nodes og de skulle gerne være 48 bytes
 - Det


 https://stackoverflow.com/questions/1972765/mmap-problem-allocates-huge-amounts-of-memory




It simplifies memory management by providing each process with a uniform address space. 

To help us keep the different caches in the memory hierarchy straight, we will use the term SRAM cache to denote the L1, L2, and L3 cache memories between the CPU and main memory, and the term DRAM cache to denote the VM system’s cache that caches virtual pages in main memory.


In virtual memory parlance, blocks are known as pages. The activity of transferring a page between disk and memory is known as swapping or paging. Pages are swapped in (paged in) from disk to DRAM, and swapped out (paged out) from DRAM to disk.


9.3.6 Locality to the Rescue Again
Although the total number of distinct pages that programs reference during an entire run might exceed the total size of physical memory, the principle of locality promises that at any point in time they will tend to work on a smaller set of active pages known as the working set or resident set. After an initial overhead where the working set is paged into memory, subsequent references to the working set result in hits, with no additional disk traffic.
As long as our programs have good temporal locality, virtual memory systems work quite well. But of course, not all programs exhibit good temporal locality. If the working set size exceeds the size of physical memory, then the program can produce an unfortunate situation known as thrashing, where pages are swapped in and out continuously. Although virtual memory is usually efficient, if a program’s performance slows to a crawl, the wise programmer will consider the possibility that it is thrashing.



This notion of mapping a set of contiguous virtual pages to an arbitrary location in an arbitrary file is known as memory mapping.

The TLBs are virtually addressed, and 4-way set associative. The L1, L2, and L3 caches are physically addressed, with a block size of 64 bytes. L1 and L2 are 8-way set associative, and L3 is 16-way set associative. The page size can be configured at start-up time as either 4 KB or 4 MB. Linux uses 4 KB pages.


As the MMU translates each virtual address, it also updates two other bits that can be used by the kernel’s page fault handler. The MMU sets the A bit, which is known as a reference bit, each time a page is accessed. The kernel can use the reference bit to implement its page replacement algorithm. The MMU sets the D bit, or dirty bit, each time the page is written to. A page that has been modified is sometimes called a dirty page. The dirty bit tells the kernel whether or not it must


Regular file in the Linux file system: An area can be mapped to a contiguous section of a regular disk file, such as an executable object file. The file section is divided into page-size pieces, with each piece containing the initial contents of a virtual page. Because of demand paging, none of these virtual pages is actually swapped into physical memory until the CPU first touches the page (i.e., issues a virtual address that falls within that page’s region of the address space). If the area is larger than the file section, then the area is padded with zeros.
https://en.wikipedia.org/wiki/Demand_paging




https://stackoverflow.com/questions/28516996/how-to-create-and-write-to-memory-mapped-files


For debugging:
http://man7.org/linux/man-pages/man2/mincore.2.html